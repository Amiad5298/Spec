Implementation Plan

Add Config Option for Agent Selection: Introduce a configuration setting or CLI argument to choose the AI agent (e.g. "agent_type"). Update fetch_config.py and settings.py to include a parameter for the desired agent (defaulting to "auggie" to preserve current behavior). This allows the user to specify "claude", "cursor", "codex", etc., instead of being locked to Auggie. For example, provide a CLI flag like --agent=claude (similar to Spec Kit’s --ai option) and load it into the settings.

Define a Base Agent Interface: Create an abstract base class in agents.py (or a new module, e.g. base_agent.py) that defines the common interface for agent interactions. This interface should include methods corresponding to each workflow step, for example: plan(spec: str) -> str (for Step 1 plan generation), generate_tasks(plan: str) -> list (Step 2 task list creation), execute_task(task: str) -> str or appropriate result (Step 3 execution of a single task, or a batch), and update_docs(context) -> str (Step 4 documentation update if applicable). It may also include initialization logic (e.g. setting up API keys or CLI paths). This base class will serve as the contract that all agent implementations (Auggie, Claude, Cursor, etc.) must fulfill.

Refactor Auggie integration into an Agent Implementation: Modify auggie.py to implement the new Agent interface (or create a new class AuggieAgent that extends the base class). Encapsulate all existing Auggie-specific logic (such as using Augment Code’s CLI/SDK calls) within this class. For example, implement plan() by calling the current planning function that uses Auggie, and similarly for generate_tasks() and others. The goal is that the rest of the system will call these methods on the agent instance, without knowing it's specifically Auggie. Ensure that any Auggie-specific configuration (like authentication or environment setup) is handled internally in this class. After this refactor, no other part of the code should call Auggie directly – only via this agent interface.

Implement Claude Agent: Create a new module/class (e.g. claude_agent.py with class ClaudeAgent implementing the base interface). This class will use Anthropic’s Claude API (or Claude CLI) to perform the same steps. For instance, ClaudeAgent.plan(spec) will formulate a prompt from the ticket specification and call the Claude API to get a plan. Likewise, implement generate_tasks(plan) by prompting Claude to break the plan into tasks, and execute_task(task) to have Claude produce code or perform the task. You will need to handle API keys (likely via an environment variable or settings.py entry for Anthropic API key) and call the Anthropic client library or HTTP requests. Make sure to format prompts and parse responses consistently with how Auggie’s outputs are expected (so the rest of the workflow can consume them). Since Claude might not have direct access to the codebase like Auggie does, ensure any required context (code files, etc.) is passed in the prompt or fetched via another mechanism (e.g., if using Auggie’s context engine via MCP as an option, though initial version can simplify by assuming the spec is self-contained).

Implement Cursor Agent: Similar to Claude, create CursorAgent class in a new module (e.g. cursor_agent.py) implementing the base interface. If Cursor provides a programmatic interface or CLI (the Spec Kit expects a cursor-agent CLI), integrate with it. For example, if there is a way to call Cursor’s AI to get a plan or complete tasks, wrap that in these methods. This might involve invoking the Cursor editor’s agent via a subprocess or through an API if available. If a direct API is not available, one approach is to use the same prompts via an OpenAI model or another backend that Cursor uses – but ideally Cursor’s own CLI can be called. In absence of a direct API, this agent could function similarly to Claude’s by using a general LLM backend (as a stand-in for Cursor’s logic). Document or handle any prerequisites (for instance, if Cursor needs to be running or logged in for its CLI to work). The CursorAgent should mirror the logic of plan -> tasks -> implement -> update docs using Cursor’s capabilities. (If Cursor cannot handle certain steps, note those limitations or rely on the base implementation fallback if any.)

Extend Ticket Fetching for Multiple Platforms: Update ticket_service.py and related fetchers to use the appropriate strategy based on the selected agent/platform. Currently, TicketService likely uses AuggieFetcher by default. Introduce logic so that if the agent is Auggie, it continues to use AuggieFetcher (which might leverage Auggie’s context engine to fetch relevant info), but if the agent is another (Claude/Cursor), use a different fetcher. This could be an existing DirectAPIFetcher (from direct_api_fetcher.py) which might fetch the ticket details directly from a source (e.g. a REST API or local file) without Auggie. Ensure TicketService.get_ticket(...) accepts a platform_override or agent parameter (as hinted by AMI-36) and passes it down to select the fetcher. You might create a factory in base.py to choose the correct fetcher subclass (for example, if agent is not Auggie, default to DirectAPIFetcher). The base fetcher interface (base.py) should remain general. This change ensures the initial specification (ticket) is retrieved properly regardless of agent. (If AuggieFetcher has unique behavior, document that it’s only used for Auggie agent.)

Integrate Agent Selection into Workflow: Modify the workflow scripts (runner.py, step1_plan.py, step2_tasklist.py, step3_execute.py, step4_update_docs.py) to utilize the new agent interface. In runner.py, instantiate the appropriate Agent based on the user’s config/choice (e.g. agent_type = settings.AGENT_TYPE; then something like agent = AgentFactory.create(agent_type) where AgentFactory returns an instance of AuggieAgent, ClaudeAgent, etc.). Pass this agent instance through to the step functions or make it accessible via a global context. Then, in each step module, replace any direct calls to Auggie logic with calls to the agent instance’s methods. For example:

In step1_plan.py, instead of calling auggie-specific planning, do plan = agent.plan(ticket_text).

In step2_tasklist.py, call tasks = agent.generate_tasks(plan).

In step3_execute.py, iterate over the task list and call agent.execute_task(task) for each (possibly in parallel, see next step).

In step4_update_docs.py, use agent.update_docs(...) if the update requires AI (or handle documentation updates accordingly).

Ensure that runner.py coordinates these calls in sequence and handles the returned results (plan text, list of tasks, code changes, etc.) exactly as before. The key is that runner and steps no longer hardcode Auggie – they use the abstract agent, making the workflow agent-agnostic.

Enable Parallel Task Execution for All Agents (Step 3): Preserve and extend the efficient parallel execution in Step 3 for Claude and Cursor. The current implementation likely executes multiple independent tasks concurrently with Auggie (perhaps by multi-threading or using asyncio). We need to implement a similar mechanism when using Claude or Cursor. For example, if the task list flags certain tasks as parallelizable (e.g. with a [P] marker or by no dependencies), we can spawn multiple threads or asynchronous calls to agent.execute_task to run them simultaneously. Implement this in step3_execute.py by grouping tasks that can run in parallel and using a thread pool or async event loop to invoke the agent on each. Make sure to collect all results and handle any exceptions. We must also consider the limitations:

Claude API: It allows multiple simultaneous requests, but we should be mindful of rate limits or token limits. Implement a short delay or limit the number of concurrent threads (e.g. 3-5 at a time) if needed to avoid hitting API limits.

Cursor: If using a local Cursor CLI or process, check if it can handle multiple requests at once. If not (e.g. if it interacts with an editor), we might have to run tasks sequentially or spin up separate instances. If CursorAgent is essentially calling an API similar to an LLM, then threads can be used similarly.

Test that parallel execution works by measuring that tasks complete faster when marked parallel. The logic should also ensure that any tasks marked as dependent run after their prerequisites (maintain order where needed, as per any dependency info in the plan). This step maintains performance optimization across all agents.

Update and Expand Tests: Refactor existing tests and add new ones to cover the multi-agent setup. For example:

In test_ticket_service.py, test that providing a platform_override (agent override) causes the TicketService to use the correct fetcher (AuggieFetcher vs DirectAPIFetcher). You might simulate a scenario for Auggie vs Claude and ensure the output (fetched ticket content) is as expected (perhaps by mocking external calls).

In test_auggie.py and test_auggie_fetcher.py, adjust them to instantiate an AuggieAgent through the new interface and verify it produces the same results as before (ensuring backwards compatibility of behavior). The tests might need to call agent.plan() or agent.execute_task() instead of whatever was done directly.

Create new tests for ClaudeAgent and CursorAgent. Since these will call external services or CLI, use mocking to simulate API responses. For ClaudeAgent, you can mock the Anthropic API call returning a sample plan or task list. For CursorAgent, if invoking a subprocess, mock the subprocess call to return a predetermined output. These tests should verify that given an input spec or plan, the agent returns outputs in the correct format.

Test the workflow end-to-end with a simple scenario for each agent: e.g. a dummy “Hello World” spec that every agent should be able to handle, and verify that the final result is correct. This ensures the integration works.
Running the full test suite should pass, confirming that multi-agent support does not break existing Auggie functionality and that new agents are integrated correctly.

Documentation and User Experience: Finally, update project documentation (README.md or usage guide) to reflect the new multi-agent support. Clearly explain how to select an agent (e.g. via config or CLI flag) and what prerequisites each agent requires (for Auggie: Augment Code account/CLI, for Claude: API key and perhaps an account, for Cursor: installing Cursor IDE or CLI, etc.). Ensure that after cloning the repository, a user can follow instructions to use any supported agent out-of-the-box. All configuration (like API keys) should be loaded automatically (e.g. via environment variables configured in settings.py). No backward compatibility concerns are needed (no existing production users), so you can remove or revise any outdated single-agent assumptions in docs. The end result is an open-source project where users can easily plug in their preferred AI agent (Auggie, Claude, Cursor, etc.) and the tool will operate seamlessly with whichever is chosen, performing the plan-task-execute-review workflow automatically.
