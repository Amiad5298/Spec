"""Aider CLI backend implementation.

This module provides the AiderBackend class that wraps the AiderClient
for use with the pluggable multi-agent architecture.

AiderBackend follows the same delegation pattern as CursorBackend:
- Extends BaseBackend to inherit shared functionality
- Implements the AIBackend protocol
- Wraps the AiderClient (delegation pattern)

Like CursorBackend, Aider has no system prompt file equivalent. Subagent
instructions are embedded directly in the user prompt (via _compose_prompt).
"""

import subprocess
import tempfile
import warnings
from collections.abc import Callable
from pathlib import Path

from ingot.config.fetch_config import AgentPlatform
from ingot.integrations.aider import (
    AiderChatMode,
    AiderClient,
    check_aider_installed,
    looks_like_rate_limit,
)
from ingot.integrations.backends.base import BaseBackend
from ingot.integrations.backends.errors import BackendTimeoutError


class AiderBackend(BaseBackend):
    """Aider CLI backend implementation.

    Extends BaseBackend to inherit shared logic (subagent parsing, model resolution).
    Wraps the AiderClient for actual CLI execution.

    Subagent instructions are embedded directly in the prompt before passing
    to the client (same approach as CursorBackend).

    Attributes:
        _client: The underlying AiderClient instance for CLI execution.
    """

    def __init__(self, model: str = "") -> None:
        """Initialize the Aider backend.

        Args:
            model: Default model to use for commands.
        """
        super().__init__(model=model)
        self._client = AiderClient(model=model)

    @property
    def name(self) -> str:
        """Return the backend name."""
        return "Aider"

    @property
    def platform(self) -> AgentPlatform:
        """Return the platform identifier."""
        return AgentPlatform.AIDER

    @property
    def supports_parallel(self) -> bool:
        """Return whether this backend supports parallel execution."""
        return True

    @property
    def supports_plan_mode(self) -> bool:
        """Aider supports plan mode via --chat-mode ask."""
        return True

    # _resolve_subagent() and _compose_prompt() inherited from BaseBackend

    @staticmethod
    def _resolve_chat_mode(
        plan_mode: bool,
        architect: bool | None,
    ) -> AiderChatMode | None:
        """Resolve the effective chat_mode from plan_mode and deprecated architect flag.

        Args:
            plan_mode: If True, maps to chat_mode="ask" for read-only mode.
            architect: Deprecated. If True, maps to chat_mode="architect".

        Returns:
            The resolved AiderChatMode, or None for default behavior.
        """
        if architect is not None:
            warnings.warn(
                "The 'architect' parameter is deprecated. "
                "Use plan_mode=True or pass chat_mode directly instead.",
                DeprecationWarning,
                stacklevel=3,
            )
            if architect:
                return "architect"

        if plan_mode:
            return "ask"

        return None

    def run_with_callback(
        self,
        prompt: str,
        *,
        output_callback: Callable[[str], None],
        subagent: str | None = None,
        model: str | None = None,
        dont_save_session: bool = False,
        timeout_seconds: float | None = None,
        plan_mode: bool = False,
        architect: bool | None = None,
    ) -> tuple[bool, str]:
        """Execute with streaming callback and optional timeout.

        Args:
            prompt: The prompt to send to Aider.
            output_callback: Callback function for streaming output.
            subagent: Optional subagent name.
            model: Optional model override.
            dont_save_session: Unused (Aider has no session persistence).
            timeout_seconds: Optional timeout in seconds (None = no timeout).
            plan_mode: If True, use --chat-mode ask for read-only mode.
            architect: Deprecated. Use plan_mode=True instead.

        Returns:
            Tuple of (success, output).

        Raises:
            BackendTimeoutError: If timeout_seconds is specified and exceeded.
        """
        chat_mode = self._resolve_chat_mode(plan_mode, architect)
        resolved_model, subagent_prompt = self._resolve_subagent(subagent, model)
        composed_prompt = self._compose_prompt(prompt, subagent_prompt)

        if timeout_seconds is not None:
            with tempfile.NamedTemporaryFile(
                mode="w", suffix=".md", delete=False, prefix="ingot_aider_"
            ) as f:
                f.write(composed_prompt)
                message_file = f.name
            try:
                cmd = self._client.build_command(
                    composed_prompt,
                    model=resolved_model,
                    message_file=message_file,
                    chat_mode=chat_mode,
                )
                exit_code, output = self._run_streaming_with_timeout(
                    cmd,
                    output_callback=output_callback,
                    timeout_seconds=timeout_seconds,
                )
                success = exit_code == 0
                return success, output
            finally:
                Path(message_file).unlink(missing_ok=True)
        else:
            return self._client.run_with_callback(
                composed_prompt,
                output_callback=output_callback,
                model=resolved_model,
                chat_mode=chat_mode,
            )

    def run_print_with_output(
        self,
        prompt: str,
        *,
        subagent: str | None = None,
        model: str | None = None,
        dont_save_session: bool = False,
        timeout_seconds: float | None = None,
        plan_mode: bool = False,
        architect: bool | None = None,
    ) -> tuple[bool, str]:
        """Run and return success status and captured output.

        Args:
            prompt: The prompt to send to Aider.
            subagent: Optional subagent name.
            model: Optional model override.
            dont_save_session: Unused (Aider has no session persistence).
            timeout_seconds: Optional timeout in seconds.
            plan_mode: If True, use --chat-mode ask for read-only mode.
            architect: Deprecated. Use plan_mode=True instead.

        Raises:
            BackendTimeoutError: If timeout_seconds is exceeded.
        """
        chat_mode = self._resolve_chat_mode(plan_mode, architect)
        resolved_model, subagent_prompt = self._resolve_subagent(subagent, model)
        composed_prompt = self._compose_prompt(prompt, subagent_prompt)
        try:
            return self._client.run_print_with_output(
                composed_prompt,
                model=resolved_model,
                timeout_seconds=timeout_seconds,
                chat_mode=chat_mode,
            )
        except subprocess.TimeoutExpired:
            raise BackendTimeoutError(
                f"Operation timed out after {timeout_seconds}s",
                timeout_seconds=timeout_seconds,
            ) from None

    def run_print_quiet(
        self,
        prompt: str,
        *,
        subagent: str | None = None,
        model: str | None = None,
        dont_save_session: bool = False,
        timeout_seconds: float | None = None,
        plan_mode: bool = False,
        architect: bool | None = None,
    ) -> str:
        """Run quietly, return output only.

        Args:
            prompt: The prompt to send to Aider.
            subagent: Optional subagent name.
            model: Optional model override.
            dont_save_session: Unused (Aider has no session persistence).
            timeout_seconds: Optional timeout in seconds.
            plan_mode: If True, use --chat-mode ask for read-only mode.
            architect: Deprecated. Use plan_mode=True instead.

        Raises:
            BackendTimeoutError: If timeout_seconds is exceeded.
        """
        chat_mode = self._resolve_chat_mode(plan_mode, architect)
        resolved_model, subagent_prompt = self._resolve_subagent(subagent, model)
        composed_prompt = self._compose_prompt(prompt, subagent_prompt)
        try:
            return self._client.run_print_quiet(
                composed_prompt,
                model=resolved_model,
                timeout_seconds=timeout_seconds,
                chat_mode=chat_mode,
            )
        except subprocess.TimeoutExpired:
            raise BackendTimeoutError(
                f"Operation timed out after {timeout_seconds}s",
                timeout_seconds=timeout_seconds,
            ) from None

    def run_streaming(
        self,
        prompt: str,
        *,
        subagent: str | None = None,
        model: str | None = None,
        timeout_seconds: float | None = None,
        plan_mode: bool = False,
    ) -> tuple[bool, str]:
        """Execute in streaming mode (non-interactive).

        Uses run_print_with_output internally.
        """
        return self.run_print_with_output(
            prompt,
            subagent=subagent,
            model=model,
            timeout_seconds=timeout_seconds,
            plan_mode=plan_mode,
        )

    def check_installed(self) -> tuple[bool, str]:
        """Check if Aider CLI is installed."""
        return check_aider_installed()

    def detect_rate_limit(self, output: str) -> bool:
        """Detect if output indicates a rate limit error."""
        return looks_like_rate_limit(output)


__all__ = [
    "AiderBackend",
]
